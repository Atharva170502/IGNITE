{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LIP_cub2rsr"
      },
      "source": [
        "<h3>Task 1 - Sentiment analysis</h3>\n",
        "In this task we are using a dataset from kaggle which is cited from the following article: <b>Malo, Pekka, et al. \"Good debt or bad debt: Detecting semantic orientations in economic texts.\" Journal of the Association for Information Science and Technology 65.4 (2014): 782-796.</b> which contains various kinds of sentences with different emotions.\n",
        "\n",
        "The task is to train a model on this data for sentiment analysis and use f-1 score as the metric to pick the best classifier model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B9Cs_4ZXWKB"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# imporing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import sklearn\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "OqFcLW8tXWKC",
        "outputId": "8ab4003b-feba-4a6d-95f2-99588494bb56"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# reading the dataset\n",
        "df = pd.read_csv(r'C:\\Users\\Atharva Tawde\\Desktop\\ignite projects\\task 1\\data_task_1.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ6_RpJA4GJD"
      },
      "source": [
        "<h3><b>Data Visualisation</b></h3>\n",
        "Below pie chart shows the distribution of the sentiment across the dataset for positive, nutral and negative. From the below pie chart we can observe that the data is likely skewed where the neutral sentiment contributes to almost 54% of the data and the negative sentiment only ocntributes for 15% of the data. This may be due to the dataset being financial in nature and most of the sentences being just stating facts about the stock market of many countries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "IQF0u87TXWKD",
        "outputId": "5c140fe1-4cc8-4801-86a9-4ee09046e835"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# plotting the sentiment distribution using a pie chart\n",
        "plt.figure(figsize=(5,5))\n",
        "df['Sentiment'].value_counts().plot(kind='pie', autopct='%1.0f%%')\n",
        "plt.title('Sentiment distribution across the dataset');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxryi1WhX-Ac",
        "outputId": "bae9eebc-7b37-4e6e-d0e1-3acc63b90805"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_lIQUcsYUst",
        "outputId": "8963ef3c-d335-4946-c8e1-83dabbaaefa8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RhSCrxl5Mtz"
      },
      "source": [
        "**Pre-processing:**\n",
        "The pre-processing of the data is done as follows:<br>\n",
        "1. Converting the text to lower case to avoid any repeat of the words.\n",
        "2. Removing the punctuations\n",
        "3. Tokenising and lemmatising the word to reduce the number of significant words that would be used in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI9R0XViXWKD"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# text preprocessing\n",
        "def pre_processing(text):\n",
        "    # converting to lower case\n",
        "    text = text.lower()\n",
        "    # removing punctuations\n",
        "    text = text.replace('[^\\w\\s]', '')\n",
        "    # removing stopwords\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = [word for word in tokens if word not in stop_words]\n",
        "    # lemmatization\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDyH3JILXWKD"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# splitting the data into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['Sentence']\n",
        "y = df['Sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMTbhx3HXWKD",
        "outputId": "68ea5f39-fe5c-4b32-ed90-b20fbbeb2e1e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# preprocessing the data\n",
        "X_train = X_train.apply(pre_processing)\n",
        "X_test = X_test.apply(pre_processing)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkSPbsdK5zQe"
      },
      "source": [
        "**Testing the models:** We have used most of the classifier models from the `sklearn` library and used the metric as f-1 score to chosse the best model to use for the task. The words are also first vectorised using the tf-idf vectorisation which will assure that the classifier models will get a numerical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUlpVrooXWKE",
        "outputId": "8acba4e1-c88c-4af0-9a92-5c268796c494"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Multinomial Naive Bayes': MultinomialNB(),\n",
        "    'Support Vector Classifier': SVC(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators = 1000,max_depth=1000),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Evaluate each classifier\n",
        "f1_scores = {}\n",
        "for name, clf in classifiers.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', tfidf),\n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    f1_scores[name] = f1\n",
        "\n",
        "# Print F1 scores for all classifiers\n",
        "print(\"F1 Scores for all classifiers:\")\n",
        "for name, score in f1_scores.items():\n",
        "    print(f\"{name}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFzbBHTSXWKE",
        "outputId": "e5395553-dec9-4583-d96a-a35dea8e4e61"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "# print the classification report for the best performing classifier\n",
        "model = max(f1_scores, key=f1_scores.get)\n",
        "print(f\"Best Classifier: {model}\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Kmj_VMspblHn",
        "outputId": "8408ba2a-0df6-4e22-9198-295c8f06d9af"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels = y.unique())\n",
        "disp.plot(cmap='BuPu',colorbar=False)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4V3zwRk7o_o"
      },
      "source": [
        "**Conclusion**: The confusion matrix for the classifiers is plotted and observed that `LogisticRegression()` model is a good classifier model with a f-1 score of approximately 67% compared to other classifier models. The classification report is also provided which tells us the f-1 scores for each of the classes where the neutral class has the highest score because of the large support size in the dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
